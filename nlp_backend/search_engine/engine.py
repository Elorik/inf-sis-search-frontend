import json
import math
from pathlib import Path
from collections import defaultdict, Counter
from typing import List, Dict, Any, Optional

class ParsedQuery:
    def __init__(self, query_text: str, tokens: List[str]):
        self.query_text = query_text
        self.tokens = tokens  # Ñ‚Ð¾ÐºÐµÐ½Ð¸ Ð·Ð°Ð¿Ð¸Ñ‚Ñƒ

class SearchEngine:
    def __init__(self, db_filename: str = "data.json"):
        self.base_dir = Path(__file__).resolve().parent.parent
        self.db_path = self.base_dir / db_filename

        print(f"ðŸ” SEARCH ENGINE: Ð’Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÑŽ Ð±Ð°Ð·Ñƒ Ð´Ð°Ð½Ð¸Ñ…: {self.db_path}")

        # inverted index: token -> {doc_id: tf}
        self.index: Dict[str, Dict[str, int]] = defaultdict(dict)

        # documents storage
        self.documents: Dict[str, Dict[str, Any]] = {}

        self._load_db()

    # ===============================
    # Ð—Ð°Ð²Ð°Ð½Ñ‚Ð°Ð¶ÐµÐ½Ð½Ñ Ð±Ð°Ð·Ð¸ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñ–Ð²
    # ===============================
    def _load_db(self):
        if not self.db_path.exists():
            print(f"Ð¤Ð°Ð¹Ð»Ñƒ {self.db_path} Ð½Ðµ Ñ–ÑÐ½ÑƒÑ”. Ð¡Ñ‚Ð²Ð¾Ñ€ÑŽÑ”Ð¼Ð¾ Ð½Ð¾Ð²Ð¸Ð¹.")
            self._save_db()
            return

        try:
            with open(self.db_path, "r", encoding="utf-8") as f:
                data = json.load(f)

            self.documents.clear()
            self.index.clear()

            for doc in data:
                doc_id = doc.get("id")
                if not doc_id:
                    continue

                self.documents[doc_id] = doc

                # Ð¢Ð¾ÐºÐµÐ½Ð¸ Ñ‚ÐµÐºÑÑ‚Ñƒ + Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ°
                tokens = []
                tokens.extend(doc.get("tokens", []))
                tokens.extend(doc.get("title_tokens", []))

                token_counts = Counter(tokens)
                for token, tf in token_counts.items():
                    self.index[token][doc_id] = tf

            print(f"âœ… Ð—Ð°Ð²Ð°Ð½Ñ‚Ð°Ð¶ÐµÐ½Ð¾ {len(self.documents)} Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñ–Ð².")

        except Exception as e:
            print(f"âŒ ÐŸÐ¾Ð¼Ð¸Ð»ÐºÐ° Ñ‡Ð¸Ñ‚Ð°Ð½Ð½Ñ Ð‘Ð”: {e}")

    # ===============================
    # Ð—Ð±ÐµÑ€ÐµÐ¶ÐµÐ½Ð½Ñ Ð‘Ð”
    # ===============================
    def _save_db(self):
        try:
            with open(self.db_path, "w", encoding="utf-8") as f:
                json.dump(list(self.documents.values()), f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âŒ ÐŸÐ¾Ð¼Ð¸Ð»ÐºÐ° Ð·Ð°Ð¿Ð¸ÑÑƒ Ð‘Ð”: {e}")

    # ===============================
    # Ð”Ð¾Ð´Ð°Ð²Ð°Ð½Ð½Ñ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°
    # ===============================
    def add_document(self, doc: Dict[str, Any]):
        doc_id = doc["id"]
        self.documents[doc_id] = doc

        tokens = []
        tokens.extend(doc.get("tokens", []))
        tokens.extend(doc.get("title_tokens", []))

        token_counts = Counter(tokens)
        for token, tf in token_counts.items():
            self.index[token][doc_id] = tf

        self._save_db()

    # ===============================
    # ÐŸÐ¾ÑˆÑƒÐº Ð· TFâ€“IDF
    # ===============================
    def search(
        self,
        query: ParsedQuery,
        doc_type_filter: str = "all",
        entity_type_filter: str = "all",
    ) -> List[Dict[str, Any]]:

        if not query.tokens:
            return []

        scores: Dict[str, float] = defaultdict(float)
        N = len(self.documents)

        # ---------- TFâ€“IDF ----------
        for token in query.tokens:
            if token not in self.index:
                continue

            df = len(self.index[token])
            idf = math.log((N + 1) / (df + 1)) + 1  # ÑÑ‚Ð°Ð±Ñ–Ð»ÑŒÐ½Ð¸Ð¹ IDF

            for doc_id, tf in self.index[token].items():
                scores[doc_id] += tf * idf

        # ---------- Ð¤Ñ–Ð»ÑŒÑ‚Ñ€Ð°Ñ†Ñ–Ñ + Ð½Ð¾Ñ€Ð¼Ð°Ð»Ñ–Ð·Ð°Ñ†Ñ–Ñ ----------
        results = []

        for doc_id, score in scores.items():
            doc = self.documents.get(doc_id)
            if not doc:
                continue

            # Ñ„Ñ–Ð»ÑŒÑ‚Ñ€ Ñ‚Ð¸Ð¿Ñƒ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°
            if doc_type_filter != "all" and doc.get("doc_type") != doc_type_filter:
                continue

            # Ñ„Ñ–Ð»ÑŒÑ‚Ñ€ ÑÑƒÑ‚Ð½Ð¾ÑÑ‚ÐµÐ¹
            if entity_type_filter != "all":
                ents = doc.get("entities", {})
                if not ents.get(entity_type_filter):
                    continue

            # Ð½Ð¾Ñ€Ð¼Ð°Ð»Ñ–Ð·Ð°Ñ†Ñ–Ñ Ð·Ð° Ð´Ð¾Ð²Ð¶Ð¸Ð½Ð¾ÑŽ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°
            doc_len = len(doc.get("tokens", [])) + len(doc.get("title_tokens", []))
            norm_score = score / max(1, doc_len)

            results.append({
                "id": doc["id"],
                "title": doc["title"],
                "snippet": doc.get("body", "")[:200] + "...",
                "score": round(norm_score, 4),
                "doc_type": doc.get("doc_type", "unknown"),
                "entities": doc.get("entities", {}),
            })

        results.sort(key=lambda x: x["score"], reverse=True)
        return results

    # ===============================
    # ÐžÑ‚Ñ€Ð¸Ð¼Ð°Ð½Ð½Ñ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°
    # ===============================
    def get_document(self, doc_id: str) -> Optional[Dict[str, Any]]:
        return self.documents.get(doc_id)

engine = SearchEngine()